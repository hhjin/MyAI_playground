{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb0735c0",
   "metadata": {},
   "source": [
    "# Few shot examples for chat models\n",
    "\n",
    "This notebook covers how to use few shot examples in chat models.\n",
    "\n",
    "There does not appear to be solid consensus on how best to do few shot prompting. As a result, we are not solidifying any abstractions around this yet but rather using existing abstractions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6e9664c",
   "metadata": {},
   "source": [
    "## Alternating Human/AI messages\n",
    "The first way of doing few shot prompting relies on using alternating human/ai messages. See an example of this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62156fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7ac3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureChatOpenAI(verbose=False, callbacks=[<langchain.callbacks.streaming_stdout.StreamingStdOutCallbackHandler object at 0x105766440>], callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='', openai_api_base='', openai_organization='', request_timeout=None, max_retries=6, streaming=True, n=1, max_tokens=3300, deployment_name='gpt35turbo-16k', openai_api_type='azure', openai_api_version='2023-03-15-preview')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../utils/')\n",
    "from mylangchainutils import QA_Toolkit \n",
    "myQAKit=QA_Toolkit(\"./Chroma_DB_300.2300\")\n",
    "chat=myQAKit.get_chat_azure(streaming=True,deployment_name=\"gpt35turbo-16k\", max_tokens=3300, temperature=0,)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98791aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eebdcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avast! I be understandin' ye, me heartie."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Avast! I be understandin' ye, me heartie.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, example_human, example_ai, human_message_prompt]\n",
    ")\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "# get a chat completion from the formatted messages\n",
    "chain.run(\"I read an write and walk away\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c4135d7",
   "metadata": {},
   "source": [
    "## System Messages\n",
    "\n",
    "OpenAI provides an optional `name` parameter that they also recommend using in conjunction with system messages to do few shot prompting. Here is an example of how to do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba92d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = SystemMessagePromptTemplate.from_template(\n",
    "    \"Hi\", additional_kwargs={\"name\": \"example_user\"}\n",
    ")\n",
    "example_ai = SystemMessagePromptTemplate.from_template(\n",
    "    \"Argh me mateys\", additional_kwargs={\"name\": \"example_assistant\"}\n",
    ")\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e488a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I be lovin' programmin', me hearty.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, example_human, example_ai, human_message_prompt]\n",
    ")\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "# get a chat completion from the formatted messages\n",
    "chain.run(\"I love programming.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
